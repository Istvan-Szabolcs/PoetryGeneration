{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14915679700107670387\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10068632535\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12334014567914866986\n",
      "physical_device_desc: \"device: 0, name: TITAN X (Pascal), pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tika import parser\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "########################################################\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Install GPU version of TF\")\n",
    "\n",
    "\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/babits_összes.pdf\n",
      "input/karinthy_összes.txt\n",
      "input/petofi_összes.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['babits_összes', 'karinthy_összes', 'petofi_összes'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs\n",
    "input_folder = 'input/'\n",
    "files = os.listdir(input_folder)\n",
    "\n",
    "input_data = {}\n",
    "\n",
    "for file in files:\n",
    "    extention = file[file.find('.'):]\n",
    "    if extention == '.pdf':\n",
    "        content = parser.from_file(input_folder+file)['content']\n",
    "        print(input_folder+file)    \n",
    "    elif extention == '.txt':\n",
    "        with open(input_folder+file, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read()\n",
    "        print(input_folder+file)  \n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR Unindetified type\")\n",
    "        break\n",
    "\n",
    "    input_data[file[:file.find('.')]] = content\n",
    "    \n",
    "\n",
    "input_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Karinthy Frigyes \n",
      "összes költeménye\n",
      "Vérmező, 795. május\n",
      "                 I\n",
      "Ó, csudacsöndes májusi éj!\n",
      "Alszik remegőn a hűs Duna tükre -\n",
      "Lent fekszik a hold a víz fenekén.\n",
      "Távol a Gellért. - Messze az éjben\n",
      "Nyulnak a sávok, zöld-feketén.\n",
      "A Várhegy fekete tömbje\n",
      "Omlik az éjbe vakon -\n",
      "Tompa rezes fény, lágy ütemekben\n",
      "Visszaverődik az ablakokon.\n",
      "Májusban az ifjú, nagy élet\n",
      "Kéjtől zokogott, kéjétől a nyárnak -:\n",
      "Éjfélben a hajóhíd alatt\n",
      "Suhanni kezdtek az árnyak.\n",
      "\n",
      "                 II\n",
      "Éjfélben halk, h\n"
     ]
    }
   ],
   "source": [
    "print(input_data['karinthy_összes'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babits_összes is added\n",
      "karinthy_összes is added\n",
      "petofi_összes is added\n"
     ]
    }
   ],
   "source": [
    "# create general tokenization: combine -> filter -> tokenization\n",
    "\n",
    "# combine texts\n",
    "\n",
    "all_texts = \"\"\n",
    "\n",
    "for k, v in input_data.items():\n",
    "    print(k, \"is added\")\n",
    "#     print(v)\n",
    "    all_texts += v\n",
    "\n",
    "with open('all_text.txt', 'w', encoding='utf-8') as w:\n",
    "    w.write(all_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE FILTERING: \t\n",
      " !\"#'()*+,-./0123456789:;?[]abcdefghijklmnopqrstuvwxyz§«»áâäèéëíîóôöúüčőű–‘’‚”„…✝\n",
      "\n",
      "\n",
      "AFTER FILTERING:\n",
      "  !\",-.?abcdefghijklmnopqrstuvwxyzáéíóöúüőű‚\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "\n",
    "filter_char = [char for char in \"()*+0123456789§«»–✝'[]#\\/:…;\"]\n",
    "# filter_char.extend([\"\\n\", \"\\t\"])\n",
    "\n",
    "replace_dict = {'á': [\"â\",\"ä\"],\n",
    "                \"é\":[\"è\", \"ë\"],\n",
    "                'í':['î'],\n",
    "                'ő':['ô'],\n",
    "                'c':['č'],\n",
    "                '\"' : ['”','„'],\n",
    "                \"'\": ['‘','’'],\n",
    "                \".\": [\"- - - - - - -- - - - - - -- - - - - - -\"],\n",
    "                \"! \":['!'],\n",
    "                \"? \":['?'],\n",
    "                \", \":[','],\n",
    "                \". \":['.'],\n",
    "                \"...\":['. . .'],\n",
    "                \"  \": [\"\\n\", \"\\t\", \"     \", \"    \", \"   \", \"   \", \"   \", \"   \"],\n",
    "                \" \":[\"  \",\"  \",\"  \"]}\n",
    "\n",
    "def filter_text(raw_text, filter_char, replace):\n",
    "    raw_text = re.sub(r'http(.*)', '', raw_text)\n",
    "    for k, v in replace.items():\n",
    "        for i in v:\n",
    "            raw_text = raw_text.replace(i,k)\n",
    "    for char in filter_char:\n",
    "        raw_text = raw_text.replace(char, '')\n",
    "    return raw_text\n",
    "\n",
    "filtered = filter_text(all_texts, filter_char, replace_dict)\n",
    "\n",
    "\n",
    "print(\"BEFORE FILTERING:\", \"\".join(sorted(set(all_texts.lower()))))\n",
    "print(\"\\n\")\n",
    "print(\"AFTER FILTERING:\\n\", \"\".join(sorted(set(filtered.lower()))))\n",
    "# print(raw_text[5480:5648])\n",
    "\n",
    "\n",
    "with open('filtered_text.txt', 'w', encoding='utf-8') as w:\n",
    "    w.write(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([filtered])\n",
    "\n",
    "# saving\n",
    "with open('models/tokenizer_magyar.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[26, 12, 10, 7, 4]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(tokenizer.texts_to_sequences([\"First\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10927 steps\n",
      "Epoch 1/50\n",
      "10927/10927 [==============================] - 149s 14ms/step - loss: 1.4968\n",
      "Epoch 2/50\n",
      "10927/10927 [==============================] - 144s 13ms/step - loss: 1.3675\n",
      "Epoch 3/50\n",
      "10927/10927 [==============================] - 143s 13ms/step - loss: 1.3750\n",
      "Epoch 4/50\n",
      "10927/10927 [==============================] - 142s 13ms/step - loss: 1.3925\n",
      "Epoch 5/50\n",
      "10927/10927 [==============================] - 144s 13ms/step - loss: 1.4113\n",
      "Epoch 6/50\n",
      "10927/10927 [==============================] - 142s 13ms/step - loss: 1.4277\n",
      "Epoch 7/50\n",
      "10927/10927 [==============================] - 142s 13ms/step - loss: 1.4451\n",
      "Epoch 8/50\n",
      "10927/10927 [==============================] - 141s 13ms/step - loss: 1.4620\n",
      "Epoch 9/50\n",
      "10922/10927 [============================>.] - ETA: 0s - loss: 1.4814\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "10927/10927 [==============================] - 141s 13ms/step - loss: 1.4814\n",
      "Epoch 10/50\n",
      "10927/10927 [==============================] - 139s 13ms/step - loss: 1.6962\n",
      "Epoch 11/50\n",
      "10927/10927 [==============================] - 140s 13ms/step - loss: 1.6361\n",
      "Epoch 12/50\n",
      "10927/10927 [==============================] - 140s 13ms/step - loss: 1.6113\n",
      "Train for 3377 steps\n",
      "Epoch 1/50\n",
      "3377/3377 [==============================] - 46s 14ms/step - loss: 1.8155\n",
      "Epoch 2/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.7105\n",
      "Epoch 3/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.6556\n",
      "Epoch 4/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.6172\n",
      "Epoch 5/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.5857\n",
      "Epoch 6/50\n",
      "3377/3377 [==============================] - 45s 13ms/step - loss: 1.5598\n",
      "Epoch 7/50\n",
      "3377/3377 [==============================] - 45s 13ms/step - loss: 1.5362\n",
      "Epoch 8/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.5252\n",
      "Epoch 9/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.5107\n",
      "Epoch 10/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4972\n",
      "Epoch 11/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4830\n",
      "Epoch 12/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4708\n",
      "Epoch 13/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4609\n",
      "Epoch 14/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4579\n",
      "Epoch 15/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4452\n",
      "Epoch 16/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4526\n",
      "Epoch 17/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4328\n",
      "Epoch 18/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4283\n",
      "Epoch 19/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4314\n",
      "Epoch 20/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4149\n",
      "Epoch 21/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.4101\n",
      "Epoch 22/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.4029\n",
      "Epoch 23/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3996\n",
      "Epoch 24/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3967\n",
      "Epoch 25/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3870\n",
      "Epoch 26/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3898\n",
      "Epoch 27/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3905\n",
      "Epoch 28/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3787\n",
      "Epoch 29/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3743\n",
      "Epoch 30/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3666\n",
      "Epoch 31/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3743\n",
      "Epoch 32/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3781\n",
      "Epoch 33/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3639\n",
      "Epoch 34/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3629\n",
      "Epoch 35/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3583\n",
      "Epoch 36/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3533\n",
      "Epoch 37/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3606\n",
      "Epoch 38/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3500\n",
      "Epoch 39/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3565\n",
      "Epoch 40/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3494\n",
      "Epoch 41/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3590\n",
      "Epoch 42/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3448\n",
      "Epoch 43/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3430\n",
      "Epoch 44/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3340\n",
      "Epoch 45/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3331\n",
      "Epoch 46/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3683\n",
      "Epoch 47/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3256\n",
      "Epoch 48/50\n",
      "3377/3377 [==============================] - 44s 13ms/step - loss: 1.3305\n",
      "Epoch 49/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3549\n",
      "Epoch 50/50\n",
      "3377/3377 [==============================] - 43s 13ms/step - loss: 1.3228\n",
      "Train for 17070 steps\n",
      "Epoch 1/50\n",
      "17070/17070 [==============================] - 223s 13ms/step - loss: 1.7605\n",
      "Epoch 2/50\n",
      "17070/17070 [==============================] - 221s 13ms/step - loss: 1.6344\n",
      "Epoch 3/50\n",
      "17070/17070 [==============================] - 221s 13ms/step - loss: 1.5981\n",
      "Epoch 4/50\n",
      "17070/17070 [==============================] - 221s 13ms/step - loss: 1.5806\n",
      "Epoch 5/50\n",
      "17070/17070 [==============================] - 221s 13ms/step - loss: 1.5669\n",
      "Epoch 6/50\n",
      "17070/17070 [==============================] - 223s 13ms/step - loss: 1.5534\n",
      "Epoch 7/50\n",
      "17070/17070 [==============================] - 222s 13ms/step - loss: 1.5421\n",
      "Epoch 8/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.5363\n",
      "Epoch 9/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.5313\n",
      "Epoch 10/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.5241\n",
      "Epoch 11/50\n",
      "17070/17070 [==============================] - 221s 13ms/step - loss: 1.5271\n",
      "Epoch 12/50\n",
      "17070/17070 [==============================] - 224s 13ms/step - loss: 1.5127\n",
      "Epoch 13/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.5090\n",
      "Epoch 14/50\n",
      "17070/17070 [==============================] - 217s 13ms/step - loss: 1.5054\n",
      "Epoch 15/50\n",
      "17070/17070 [==============================] - 222s 13ms/step - loss: 1.5006\n",
      "Epoch 16/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4960\n",
      "Epoch 17/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4976\n",
      "Epoch 18/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4960\n",
      "Epoch 19/50\n",
      "17070/17070 [==============================] - 218s 13ms/step - loss: 1.4971\n",
      "Epoch 20/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.4888\n",
      "Epoch 21/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4916\n",
      "Epoch 22/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4859\n",
      "Epoch 23/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.4814\n",
      "Epoch 24/50\n",
      "17070/17070 [==============================] - 222s 13ms/step - loss: 1.4773\n",
      "Epoch 25/50\n",
      "17070/17070 [==============================] - 225s 13ms/step - loss: 1.4819\n",
      "Epoch 26/50\n",
      "17070/17070 [==============================] - 222s 13ms/step - loss: 1.4788\n",
      "Epoch 27/50\n",
      "17070/17070 [==============================] - 222s 13ms/step - loss: 1.4739\n",
      "Epoch 28/50\n",
      "17070/17070 [==============================] - 221s 13ms/step - loss: 1.4704\n",
      "Epoch 29/50\n",
      "17070/17070 [==============================] - 223s 13ms/step - loss: 1.4726\n",
      "Epoch 30/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4692\n",
      "Epoch 31/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.4715\n",
      "Epoch 32/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.4727\n",
      "Epoch 33/50\n",
      "17070/17070 [==============================] - 222s 13ms/step - loss: 1.4694\n",
      "Epoch 34/50\n",
      "17070/17070 [==============================] - 224s 13ms/step - loss: 1.4688\n",
      "Epoch 35/50\n",
      "17070/17070 [==============================] - 224s 13ms/step - loss: 1.4704\n",
      "Epoch 36/50\n",
      "17070/17070 [==============================] - 218s 13ms/step - loss: 1.4656\n",
      "Epoch 37/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4611\n",
      "Epoch 38/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4610\n",
      "Epoch 39/50\n",
      "17070/17070 [==============================] - 218s 13ms/step - loss: 1.4630\n",
      "Epoch 40/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4623\n",
      "Epoch 41/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.4581\n",
      "Epoch 42/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4600\n",
      "Epoch 43/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4574\n",
      "Epoch 44/50\n",
      "17070/17070 [==============================] - 218s 13ms/step - loss: 1.4564\n",
      "Epoch 45/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4572\n",
      "Epoch 46/50\n",
      "17070/17070 [==============================] - 219s 13ms/step - loss: 1.4623\n",
      "Epoch 47/50\n",
      "17070/17070 [==============================] - 224s 13ms/step - loss: 1.4538\n",
      "Epoch 48/50\n",
      "17070/17070 [==============================] - 224s 13ms/step - loss: 1.4540\n",
      "Epoch 49/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.4684\n",
      "Epoch 50/50\n",
      "17070/17070 [==============================] - 220s 13ms/step - loss: 1.4529\n"
     ]
    }
   ],
   "source": [
    "# max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "# dataset_size = tokenizer.document_count # total number of characters\n",
    "#max_id#, dataset_size\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 52\n",
    "n_steps = 100\n",
    "\n",
    "# train_size = len(encoded) * 90 // 100\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, batch_size],\n",
    "                     # no dropout in stateful RNN (https://github.com/ageron/handson-ml2/issues/32)\n",
    "                     # dropout=0.2, recurrent_dropout=0.2,\n",
    "                     ),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     # dropout=0.2, recurrent_dropout=0.2\n",
    "                    ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(batch_size, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TOKENIZE THE GIVEN INPUT SET\n",
    "for poet, poems in input_data.items():\n",
    "    filtered_input = filter_text(poems, filter_char, replace_dict)\n",
    "    [encoded] = np.array(tokenizer.texts_to_sequences([filtered_input])) - 1\n",
    "    train_size = len(encoded) * 90 // 100\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "    window_length = n_steps + 1 \n",
    "    dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    dataset= dataset.shuffle(10000).batch(batch_size)\n",
    "    dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:, 1:]))\n",
    "\n",
    "    dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=batch_size), Y_batch))\n",
    "    dataset = dataset.prefetch(1)\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=10, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(poet + '.mdl_wts.hdf5', save_best_only=True, monitor='loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "    history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=50, callbacks=[earlyStopping, mcp_save,reduce_lr_loss])\n",
    "    model.save('models/' + poet + \"_model.h5\")\n",
    "    \n",
    "#     for X_batch, Y_batch in dataset.take(1):\n",
    "#         print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the full text (substract 1 to get IDs from 0-38 rather than 1-39)\n",
    "# [encoded] = np.array(tokenizer.texts_to_sequences([raw_text])) - 1\n",
    "# train_size = dataset_size * 90 // 100\n",
    "# train_size = len(encoded) * 90 // 100\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "# [encoded] = np.array(tokenizer.texts_to_sequences([raw_text])) -1\n",
    "\n",
    "# n_steps = 100\n",
    "# window_length = n_steps + 1 \n",
    "# dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "# # dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "# dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# batch_size = 50\n",
    "# dataset= dataset.shuffle(10000).batch(batch_size)\n",
    "# dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:, 1:]))\n",
    "\n",
    "# dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "# dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax')),\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "# history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(dataset, epochs = 20)\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Babits_2_GRU_1_TimeDist_Dense_no_dropout\"\n",
    "# !mkdir -p model_name\n",
    "# model.save(model_name)\n",
    "\n",
    "# new_model = tf.keras.models.load_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USING THE MODEL TO PRED CHAR\n",
    "# def preprocess(texts):\n",
    "#     X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "#     return tf.one_hot(X, max_id)\n",
    "\n",
    "# X_new = preprocess([\"ej mi a k\"])\n",
    "# Y_pred = model.predict_classes(X_new)\n",
    "# tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def next_char(text, temperature=1):\n",
    "#     X_new = preprocess([text])\n",
    "#     y_proba = model.predict(X_new)[0, -1:, :]\n",
    "#     rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "#     char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "#     return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# next_char(\"valam\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def complete_text(text, n_chars=50, temperature=1):\n",
    "#     for _ in range(n_chars):\n",
    "#         text += next_char(text, temperature)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "\n",
    "# # print(complete_text(\"t\", temperature=0.2))\n",
    "# print(complete_text(\"Magyar \",n_chars=250 ,temperature=0.8))\n",
    "# # print(complete_text(\"t\", temperature=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
