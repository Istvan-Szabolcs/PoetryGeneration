{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tika import parser\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "########################################################\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "input_folder = 'input/'\n",
    "files = os.listdir(input_folder)\n",
    "\n",
    "input_data = {}\n",
    "\n",
    "for file in files:\n",
    "    extention = file[file.find('.'):]\n",
    "    if extention == '.pdf':\n",
    "        content = parser.from_file(input_folder+file)['content']\n",
    "        print(input_folder+file)    \n",
    "    elif extention == '.txt':\n",
    "        with open(input_folder+file, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read()\n",
    "        print(input_folder+file)  \n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR Unindetified type\")\n",
    "        break\n",
    "\n",
    "    input_data[file[:file.find('.')]] = content\n",
    "    \n",
    "\n",
    "input_data.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_data['karinthy_összes'][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create general tokenization: combine -> filter -> tokenization\n",
    "\n",
    "# combine texts\n",
    "\n",
    "all_texts = \"\"\n",
    "\n",
    "for k, v in input_data.items():\n",
    "    print(k, \"is added\")\n",
    "#     print(v)\n",
    "    all_texts += v\n",
    "\n",
    "with open('all_text.txt', 'w', encoding='utf-8') as w:\n",
    "    w.write(all_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter\n",
    "\n",
    "filter_char = [char for char in \"()*+0123456789§«»–✝'[]#\\/:…;\"]\n",
    "# filter_char.extend([\"\\n\", \"\\t\"])\n",
    "\n",
    "replace_dict = {'á': [\"â\",\"ä\"],\n",
    "                \"é\":[\"è\", \"ë\"],\n",
    "                'í':['î'],\n",
    "                'ő':['ô'],\n",
    "                'c':['č'],\n",
    "                '\"' : ['”','„'],\n",
    "                \"'\": ['‘','’'],\n",
    "                \".\": [\"- - - - - - -- - - - - - -- - - - - - -\"],\n",
    "                \"! \":['!'],\n",
    "                \"? \":['?'],\n",
    "                \", \":[','],\n",
    "                \". \":['.'],\n",
    "                \"...\":['. . .'],\n",
    "                \"  \": [\"\\n\", \"\\t\", \"     \", \"    \", \"   \", \"   \", \"   \", \"   \"],\n",
    "                \" \":[\"  \",\"  \",\"  \"]}\n",
    "\n",
    "def filter_text(raw_text, filter_char, replace):\n",
    "    raw_text = re.sub(r'http(.*)', '', raw_text)\n",
    "    for k, v in replace.items():\n",
    "        for i in v:\n",
    "            raw_text = raw_text.replace(i,k)\n",
    "    for char in filter_char:\n",
    "        raw_text = raw_text.replace(char, '')\n",
    "    return raw_text\n",
    "\n",
    "filtered = filter_text(all_texts, filter_char, replace_dict)\n",
    "\n",
    "\n",
    "print(\"BEFORE FILTERING:\", \"\".join(sorted(set(all_texts.lower()))))\n",
    "print(\"\\n\")\n",
    "print(\"AFTER FILTERING:\\n\", \"\".join(sorted(set(filtered.lower()))))\n",
    "# print(raw_text[5480:5648])\n",
    "\n",
    "\n",
    "with open('filtered_text.txt', 'w', encoding='utf-8') as w:\n",
    "    w.write(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([filtered])\n",
    "\n",
    "# saving\n",
    "with open('models/tokenizer_magyar.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(tokenizer.texts_to_sequences([\"First\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "# dataset_size = tokenizer.document_count # total number of characters\n",
    "#max_id#, dataset_size\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 52\n",
    "n_steps = 100\n",
    "\n",
    "# train_size = len(encoded) * 90 // 100\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, batch_size],\n",
    "                     # no dropout in stateful RNN (https://github.com/ageron/handson-ml2/issues/32)\n",
    "                     # dropout=0.2, recurrent_dropout=0.2,\n",
    "                     ),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     # dropout=0.2, recurrent_dropout=0.2\n",
    "                    ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(batch_size, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TOKENIZE THE GIVEN INPUT SET\n",
    "for poet, poems in input_data.items():\n",
    "    filtered_input = filter_text(poems, filter_char, replace_dict)\n",
    "    [encoded] = np.array(tokenizer.texts_to_sequences([filtered_input])) - 1\n",
    "    train_size = len(encoded) * 90 // 100\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "    window_length = n_steps + 1 \n",
    "    dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    dataset= dataset.shuffle(10000).batch(batch_size)\n",
    "    dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:, 1:]))\n",
    "\n",
    "    dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=batch_size), Y_batch))\n",
    "    dataset = dataset.prefetch(1)\n",
    "    \n",
    "    earlyStopping = EarlyStopping(monitor='loss', patience=10, verbose=0, mode='min')\n",
    "    mcp_save = ModelCheckpoint(poet + '.mdl_wts.hdf5', save_best_only=True, monitor='loss', mode='min')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "    history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=50, callbacks=[earlyStopping, mcp_save,reduce_lr_loss])\n",
    "    model.save('models/' + poet + \"_model.h5\")\n",
    "    \n",
    "#     for X_batch, Y_batch in dataset.take(1):\n",
    "#         print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the full text (substract 1 to get IDs from 0-38 rather than 1-39)\n",
    "# [encoded] = np.array(tokenizer.texts_to_sequences([raw_text])) - 1\n",
    "# train_size = dataset_size * 90 // 100\n",
    "# train_size = len(encoded) * 90 // 100\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "# [encoded] = np.array(tokenizer.texts_to_sequences([raw_text])) -1\n",
    "\n",
    "# n_steps = 100\n",
    "# window_length = n_steps + 1 \n",
    "# dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "# # dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "# dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "\n",
    "# batch_size = 50\n",
    "# dataset= dataset.shuffle(10000).batch(batch_size)\n",
    "# dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:, 1:]))\n",
    "\n",
    "# dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "# dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax')),\n",
    "# ])\n",
    "\n",
    "\n",
    "\n",
    "# history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(dataset, epochs = 20)\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"Babits_2_GRU_1_TimeDist_Dense_no_dropout\"\n",
    "# !mkdir -p model_name\n",
    "# model.save(model_name)\n",
    "\n",
    "# new_model = tf.keras.models.load_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USING THE MODEL TO PRED CHAR\n",
    "# def preprocess(texts):\n",
    "#     X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "#     return tf.one_hot(X, max_id)\n",
    "\n",
    "# X_new = preprocess([\"ej mi a k\"])\n",
    "# Y_pred = model.predict_classes(X_new)\n",
    "# tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def next_char(text, temperature=1):\n",
    "#     X_new = preprocess([text])\n",
    "#     y_proba = model.predict(X_new)[0, -1:, :]\n",
    "#     rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "#     char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "#     return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# next_char(\"valam\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def complete_text(text, n_chars=50, temperature=1):\n",
    "#     for _ in range(n_chars):\n",
    "#         text += next_char(text, temperature)\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "\n",
    "# # print(complete_text(\"t\", temperature=0.2))\n",
    "# print(complete_text(\"Magyar \",n_chars=250 ,temperature=0.8))\n",
    "# # print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
