{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "########################################################\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "\n",
    "\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"input/petofi_összes.txt\"\n",
    "\n",
    "with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Szabadság, szerelem!\n",
      "E kettő kell nekem,\n",
      "Szerelmemért feláldozom\n",
      "Az életet,\n",
      "Szabadságért feláldozom\n",
      "Szerelmemet.\n",
      "\n",
      "\n",
      "Tüzesen süt le a nyári nap sugár\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !()*,-.0123456789:;?abcdefghijklmnopqrstuvwxyz§«»áéëíóöúüőű–‘’‚…✝'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(raw_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "búsabb nótáját; A harmat, mely ekkor ellepett fűt, bokort, Tán a szánakozó csillagok könnye volt. Iluska már alutt. A pitvar eleje Volt nyár idejében rendes fekvőhelye,\n"
     ]
    }
   ],
   "source": [
    "filtered = [char for char in \"()0123456789§«»–…✝'\"]\n",
    "\n",
    "\n",
    "for char in filtered:\n",
    "    raw_text = raw_text.replace(char, '')\n",
    "\n",
    "raw_text = raw_text.replace('\\n', ' ')\n",
    "\n",
    "print(raw_text[5480:5648])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([raw_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[26, 14, 11, 7, 4]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts(tokenizer.texts_to_sequences([\"First\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a reverse dictionary\n",
    "# reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# # Function takes a tokenized sentence and returns the words\n",
    "# def sequence_to_text(list_of_indices):\n",
    "#     # Looking up words in dictionary\n",
    "#     words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "#     return(words)\n",
    "\n",
    "\n",
    "# sequence_to_text([50, 10, 8, 7, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters\n",
    "\n",
    "max_id, dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the full text (substract 1 to get IDs from 0-38 rather than 1-39)\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([raw_text])) - 1\n",
    "# train_size = dataset_size * 90 // 100\n",
    "train_size = len(encoded) * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "# [encoded] = np.array(tokenizer.texts_to_sequences([raw_text])) -1\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 \n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "# dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 48\n",
    "dataset= dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:,:-1], windows[:, 1:]))\n",
    "\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 100, 48) (48, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 18538 steps\n",
      "Epoch 1/10\n",
      "18538/18538 [==============================] - 208s 11ms/step - loss: 1.2863\n",
      "Epoch 2/10\n",
      "18538/18538 [==============================] - 203s 11ms/step - loss: 1.2403\n",
      "Epoch 3/10\n",
      "18538/18538 [==============================] - 205s 11ms/step - loss: 1.2804\n",
      "Epoch 4/10\n",
      "18538/18538 [==============================] - 205s 11ms/step - loss: 1.3141\n",
      "Epoch 5/10\n",
      "18538/18538 [==============================] - 205s 11ms/step - loss: 1.3439\n",
      "Epoch 6/10\n",
      "18538/18538 [==============================] - 208s 11ms/step - loss: 1.3697\n",
      "Epoch 7/10\n",
      "18538/18538 [==============================] - 204s 11ms/step - loss: 1.3992\n",
      "Epoch 8/10\n",
      "18538/18538 [==============================] - 203s 11ms/step - loss: 1.4246\n",
      "Epoch 9/10\n",
      "18538/18538 [==============================] - 205s 11ms/step - loss: 1.4308\n",
      "Epoch 10/10\n",
      "18538/18538 [==============================] - 207s 11ms/step - loss: 1.4679\n"
     ]
    }
   ],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "#     keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax')),\n",
    "# ])\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     # no dropout in stateful RNN (https://github.com/ageron/handson-ml2/issues/32)\n",
    "                     # dropout=0.2, recurrent_dropout=0.2,\n",
    "                     ),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     # dropout=0.2, recurrent_dropout=0.2\n",
    "                    ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.compile(loss=keras.losses.sparse_categorical_crossentropy, optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(dataset, epochs = 20)\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\szabo\\AppData\\Local\\conda\\conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: Petöfi_2_GRU_1_TimeDist_Dense_no_dropout\\assets\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Petöfi_2_GRU_1_TimeDist_Dense_no_dropout\"\n",
    "# !mkdir -p model_name\n",
    "model.save(model_name)\n",
    "\n",
    "# new_model = tf.keras.models.load_model('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USING THE MODEL TO PRED CHAR\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "X_new = preprocess([\"ej mi a k\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "# tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "next_char(\"valam\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zsuzsanna magad gyönyörű tekinté; futozátom éj. ha rátomatoztélett reám. egy ektrapáktól körűl! egy magad.-- éjet tánczolott megállt meg bujnokad, lassan ütneiik. király eszedben kiomlocs fekszeket a királyogat\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# print(complete_text(\"t\", temperature=0.2))\n",
    "print(complete_text(\"Zsuzsanna \",n_chars=200 ,temperature=0.9))\n",
    "# print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
